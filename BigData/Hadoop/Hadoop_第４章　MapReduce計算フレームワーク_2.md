# 分散大規模データ処理システム -- Hadoop-6

# 第４章　MapReduce計算フレームワーク-2/3

　ここでは、MapReduceの実行原理について説明する。

## 第７節　MapReduceの原理分析

　MapReduce全体の処理流れは、MapTaskとReduceTaskに分けて考えることができ、本章ではまずMapTask側から順に解説を進めていく。

### 7.1　MapTask運行仕組み

![img](D:\OneDrive\picture\Typora\BigData\Hadoop\a9b2a382aae117feefb7706a65771940.png)

**Read段階**

- まず、対象ファイルは読み込まれる前に、ローカルでブロック（block）に分割されて各ノードに割り当てられる。通常、128MBの固定サイズに従って分割し、残る部分が128MBに満たない場合でも、1つのブロックとして処理される。
- データを読み込む時には、`getSplits()` メソッドを使用して、ブロックを論理的な単位であるス切片（split）に分割する。この分割は物理的ではなく理論上の切り分けであり、デフォルトでは切片のサイズは 128MBに設定されており、HDFSのブロックサイズと同じです。そのため、通常の設定では切片とブロックは一対一関係です。そして、切片の数に応じて、同数のMapTaskが後続の段階で起動される。
- `InputFormat`クラスを継承する`FileInputFormat`が、`getSplits()`メソッドを実装して、入力ファイルを切り分ける。この切り分けは論理的なものであり、物理的に分割するわけではない。

![image-20240923114610607](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20240923114610607.png)

- 切り分けされたsplit情報、実行対象のコードを含むJARファイル、さらにジョブ（Job）の実行に必要な設定情報（XML設定ファイルなど）は、1つのジョブパッケージとして纏められ、最終的に YARN に提出される。
- YARNは、この提出されたジョブに対して、実行に必要なリソース（メモリやCPUなど）を割り当てる。次にMrAppMaster（MapReduce Application Master）と呼ばれるプロセスを起動し、ジョブの生命周期を管理する。MrAppMasterは、処理の実行順序を決定し、その制御のもとでMapTaskおよびReduceTaskが順次実行され、ジョブ処理を完了する。

![image-20241223075836684](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20241223075836684.png)

**Map段階**

- Map段階に実行されるロジックは上書きされた`map()`メソッドが担当する。
- 入力の`<key/value>`ペアは、行番号（key）とその行の文字列（Value）です。出力の`<key/value>`ペアは単語（key） と その単語の出現回数1（Value）です。つまり、入力の `<key: 行番号 / value: 行の文字列>` が、`<key: 単語 / value: 出現回数1>` という形式に変換される。

![image-20241014162847885](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20241014162847885.png)

- map処理が完了すると、各mapの結果は `context.write()` を通じてフレームワークに送られ、収集される。

**Collect段階**

- map処理が完了すると、各mapの結果は `context.write()` を通じてフレームワークに渡され、データが収集されます。これらのデータは、後続処理に備えて一時的にキャッシュへ格納される。
- しかし、その前に分類処理が行われる。具体的には、データのkeyをハッシュ化（Hash）し、その値をReduceタスクの数で割って余りを求める。同じ余りデータを同じRedcuerタスクに割り当てる。
- その結果は、1つのMapTaskから出力された`<key：単語/value：単語数>`ペアが、同じ余り値を持つものはこの段階で既に纏められる。これは、Reduce タスク間の処理負荷を均等化するための重要なステップでもある。

- その一時キャッシュが環状キャッシュと呼ばれる。本質には2つ配列から構成されており、1つ目の配列には`<key：単語/value：出現回数>`ペアを格納する。2つ目の配列にはそれらのペアに対応する索引が格納されている。図の1つ目に示されている、2つの半円形の矢印が組み合わさったような記号が、この環状構造を表している。

![image-20250113204739841](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20250113204739841.png)

- 環状キャッシュは、デフォルト100MBサイズ制限がある。データを書き込む時に、使用率が閾値（80%）に達すると、その領域はロックされ、新規の書き込みは行わない。
- このタイミングで、別のプロセスがキャッシュからデータを取り出し、別の場所へ転送する。同時に書き込み処理が配列の末尾から逆方向に残りの20MBキャッシュから続ける。そして、再び80%閾値に達すると、上述の処理を繰り返し、今度は先頭から書き込みが再開される。
- 環状キャッシュが上述の流れを抽象化する構造であり、新しいデータの追加と古いデータの削除が同時に行われるため、効率的なデータ処理が可能になる。

**Spill段階**

- Spillとは、上述の環状キャッシュからデータを書き出す処理のことです。環状キャッシュの使用率80%閾値に達すると、80MB分のデータ領域がロックされ、専門のプロセスを起動する。このプロセスは、ロックされたデータをキャッシュから取り出し、一時ファイル（temporary file）に書き込む。

> `spill` は漢字で表すと「溢出」や「溢写」といった表記になり、意味をイメージしやすくなる。いずれも「容量がいっぱいになったデータを外部へ書き出す」という語感を持っている。

![image-20241225075625672](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20241225075625672.png)

- 臨時ファイルに出力する前にデータに並べ替えをする。現在のデータは同じの余りを持つ`<key：単語/value：単語数>`値が混ぜて環形キャッシュに格納られる。
- 赤枠の流れは臨時ファイルに出力する時に現在のデータを並び替える。その済んだら同じのkeyを持つ`<key：単語/value：単語数>`が集められる（⑧）。今は、正直パーティション（分区）が成り立ったばかりだと思う。
- 並び替えられてパーティションになってしまうデータは臨時ファイルに書き込む（⑨）。パーティションに比べて出力サイズが割と大きい場合、複数の臨時ファイルが存在するになる。
- 次は各パーティションを併合するMerge階段に入る。

**Merge階段**

- そして、今回MapTaskの全てのデータ処理が終了した後に、ディスク上の一時ファイルを併合（merge）して一つにまとめます。
- 最終的には一つのファイルのみがディスクに書き込まれ、このファイルには各ReduceTaskに対応するデータのオフセット（offset）を記録するための索引も提供されます。

![image-20241225075713510](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20241225075713510.png)

　ここまでMapTaskの流れが終わりました。

### 7.2　MapTask並列処理

　MapTaskの並列処理とは、同時に実行されるMapTaskのタスク数を指し、Mapフェーズの並列処理能力を測る指標となります。この並列度を決定する要素は、read段階で、FileInputFormat`クラスの`getSplits()`メソッドが呼び出され、データブロックを論理的に分割して生成された切片（スプリット）数です。デフォルト場合で、切片のサイズとデータブロックのサイズが一致しています。このため、以下等式が成り立ちます：

- データブロックの数 ＝ 切片の数 ＝ MapTaskの数

　ここで例を挙げて切片を説明しましょう。

![image-20250216204508212](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20250216204508212.png)

　図1には、切片の大小とブロックが同じ、ブロックの全ては何も変わらなくて一つのタスクに入れて処理をします。図2は、100Mを単位として128Mデータが100Mと28Mに分けられました。残りの28Mデータが100Mに揃えておくため次のブロックに移動することが必要です。その過程には、最終タスク数が変わってないけど、データの移動があると余分な資源を消費するになります。

　その例を挙げて話したいのは切片大小を減ってタスク数が増えても実際に効率的になりません。ただ、ブロックと切片大小を一緒に縮めてタスク数は増えすぎを防ぐ上に適切に並列度を高めるとは、効率的にすることができます。

```
# hdfs-default.xml設定
dfs.blocksize=128MB

# mapred-default.xml配置ファイル
mapreduce.input.fileinputformat.split.maxsize=256MB
mapreduce.input.fileinputformat.split.minsize=64MB
```

　次、ソースコードを見て切片大小の判定ルールを紹介する。

　ブロックサイズが直接配置ファイルから得ってデフォルト値が128M、setBlockSize()みたいメソッドがないので、コードレベルに一時的に変更することができない。

![image-20250309112553210](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20250309112553210.png)

　切片サイズには、ブロック、切片の最大値と切片の最小値を比べて一つを選んで決める。computeSplitSizeメソッドから見えてブロックサイズをmaxSizeとminSize範囲を過ぎるにしては意味なく、決してその範囲にいる。その仕組みは切片サイズ範囲を制御して、多すぎまたは少なすぎタスクを生まれるせいで効率が下がることを防ぎ、ただその場合はデータ移動が生じてある。つまり、範囲内にの切片がブロック自身であり、範囲にブロックサイズを変えてタスク数が増えるとデータの移動もない。

![image-20250309112059668](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20250309112059668.png)

![image-20250309120204543](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20250309120204543.png)

　極端な状況に大規模データ、あるいは大量の小さなファイルに会うと、多すぎタスク数が生じるになる。block、minSize、maxSize三つを一緒に変えて、タスク数を減える同時にデータを移動することも起こってない。

![image-20250320161536287](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20250320161536287.png)

　赤枠にソースコード表れるのは`bytesRemaining` が `splitSize` (切片サイズ) の 1.1 倍より大きい場合にのみ、一つの切片として分割される。ならば129Mのファイルも一つのブロックに入れる。



　ここにMapReduceにとって1つの重要な特性が見える。ロジックサイズを切片（spill）サイズに割ると複数のMapTaskをなすから、MapTaskにデータはMapTask数に割るとパーティションをなすまで、その流れにMapTask数、パーティション数は一致です。最初のMapTask一旦決まったら最後までその並行度で動いている。MapReduceの実行方案はその特性に合うかどうかは効率にかなり影響してくる。一方的に並行度を増加すると逆に効率的じゃなくなる。

　MapReduceの前半Map流れの解説は終わり、次は後半Reduce流れの流れです。よろしくお願いいたします。
