# 分散大規模データ処理システム -- Hadoop-6

# 第４章　MapReduce計算フレームワーク-2/2

　第４章には、主にMapReduceの運行原理に関する紹介をします。

## 第７節　MapReduceの原理分析

　全体のMapReduce流れがMapTaskとReduceTaskに分けておきて紹介を進めます。

### 7.1　MapTask運行仕組み

![img](D:\OneDrive\picture\Typora\BigData\Hadoop\a9b2a382aae117feefb7706a65771940.png)

**Read階段**

- まず、目標ファイルを読み込む前にローカルでブロック（block）に切り分けて各節点に割り当てます。一般的には128MB固定長に従って分割し、特別のルールがありません。
- データを読み込む時にgetSplits() メソッドを使用してブロックを切片（splits）に切り分けます。それは理論的な切り分け、デフォルト場合でsplitsの大小は128MBで、ブロックのデフォルト大小と同じです。即ち、デフォルト場合でsplitとblock関係が一対一です。あと、splitsの数量に応じて次の階段に同様なMapTask数を起動されてあります。
- InputFormatクラスを継承するFileInputFormatが、getSplits()メソッドを実装してファイルの切り分けをします。この切り分けは論理的（ロジック）で、物理的な切り分けではありません。

![image-20240923114610607](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20240923114610607.png)

- 切り分けされたのsplit情報、実行するコードを含むjarファイル、そしてジョブ（Job）実行に必要な設定情報（XML設定ファイルなど）を作成します。この一連の情報がまとめられ、YARNに提出されます。
- YARNは、この提出されたジョブに対して、ジョブを実行する必要なリソース（メモリ、CPUなど）を割り当てます。YARNはまず MrAppMaster（MapReduce Application Master）という実体を起動して、ジョブの周期を管理し、流れの実行順序を制定します。後のMapTask、ReduceTaskにジョブ処理がMrAppMasterの制御で仕上げます。

![image-20241223075836684](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20241223075836684.png)

**Map階段**

- Map階段に実行するロジックは書き直されたmap()メソッドが担当します。
- 入力の<key/value>値はドキュメントの行数keyと当の行の文字です。出力の<key/value>値が一つ単語textとその単語の計数1です。入力<key/value>値が新しい<key/value>値に転換になります。

![image-20241014162847885](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20241014162847885.png)

- mapの処理が完了した後、mapの各結果は`context.write`を通じてデータが収集されます。

**Collect階段**

- mapの処理が完了した後、mapの各結果は`context.write()`を通してデータが収集されます。それらのデータをキャッシュに一時的に格納します。
- でもその前に分類処理があります。データのkey値をハッシュ化（Hash）し、Reduceタスクの数で割って余りを取り、同じ余りによってデータを各タスクに割り当てます。
- そのために一つのMapTaskジョブに同じkeyを持つ<key/value>値が纏められます。後のデータ併合ReduceTaskがやすくなれます。その同時に全体の処理負荷を均等化することもできます。
- その一時的なキャッシュが環形キャッシュと呼ばれます。本質は配列で、key/valueペアとそのペアに対応する索引がそれぞれ格納されています。図中の2つの半円形矢印記号みたいものです。

![image-20250113204739841](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20250113204739841.png)

- 環形キャッシュは、デフォルト100Mサイズの制御があります。データを書き込む時に、一定の占有率（80%）に達したらこの部分がロックされて書き込みません。新しいプロセスがデータをキャッシュから取り出して別の所に転送します。その同時に書き込むプロセスが続きます。ただ、配列の末尾から逆方向に残りの20Mキャッシュに書き込みます。一旦80%閾値に達したら上記の過程を繰り返して先頭から書き込みます。
- 環形キャッシュが上述流れを抽象化にする表示であり、新しいデータの追加と古いデータの削除が同時に行われるため効率的になれます。

**Spill階段**

- Spill実は上記の環形キャッシュから書き出す流れです。環形キャッシュの80%閾値までになって80Mデータをロックして単独のプロセスを起動します。データを取り出して一時ファイルに書き込みます。

> spillが漢字で表せば「溢出」にでき、或いは「溢写」に書くと理解しやすい。

![image-20241225075625672](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20241225075625672.png)

- 臨時のファイルに書き込む前に、幾つかデータ処理があります。先ず、上図のパーティション1とパーティション2がkeyによって分けて出します。一つのパーティション（partition）が一種単語keyを代表します。例えば、パーティション1が今回Spill溢出の<a,1>を格納し、パーティション2が<b,1>を格納します。
- 次、さっき分けられたパーティションがkey/value値の索引によって並べ替えをします。
- 並べ替えられたデータを一時ファイルに書き込み、次のMerge階段に入ります。
- もしあるパーティションに対して出力結果が非常に大きい場合、ディスク上には複数の一時ファイルが存在することになります。

**Merge階段**

- そして、今回MapTaskの全てのデータ処理が終了した後に、ディスク上の一時ファイルを併合（merge）して一つにまとめます。
- 最終的には一つのファイルのみがディスクに書き込まれ、このファイルには各ReduceTaskに対応するデータのオフセット（offset）を記録するための索引も提供されます。

![image-20241225075713510](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20241225075713510.png)

　ここまでMapTaskの流れが終わりました。

### 7.2　MapTask並列処理

