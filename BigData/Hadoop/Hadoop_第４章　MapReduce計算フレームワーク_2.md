# 分散大規模データ処理システム -- Hadoop-6

# 第４章　MapReduce計算フレームワーク-2/3

　MapReduceにのMapTask運行原理について紹介する。

## 第７節　MapReduceの原理分析

　全体のMapReduce流れがMapTaskとReduceTaskに分けておきて紹介を進める。

### 7.1　MapTask運行仕組み

![img](D:\OneDrive\picture\Typora\BigData\Hadoop\a9b2a382aae117feefb7706a65771940.png)

**Read階段**

- まず、目標ファイルを読み込む前にローカルでブロック（block）に切り分けて各節点に割り当てる。一般的には128MB固定大小に従って分割し、残り不足するなら一つのブロックとして処理する。
- データを読み込む時にgetSplits() メソッドを使用してブロックを切片（splits）に切り分けます。それは理論的な切り分け、デフォルト場合でsplitsの大小は128MBで、ブロックのデフォルト大小と同じです。即ち、デフォルト場合でsplitとblock関係が一対一です。あと、splitsの数量に応じて次の階段に同様なMapTask数を起動されてあります。
- InputFormatクラスを継承するFileInputFormatが、getSplits()メソッドを実装してファイルの切り分けをします。この切り分けは論理的（ロジック）で、物理的な切り分けではありません。

![image-20240923114610607](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20240923114610607.png)

- 切り分けされたのsplit情報、実行するコードを含むjarファイル、そしてジョブ（Job）実行に必要な設定情報（XML設定ファイルなど）を作成します。この一連の情報がまとめられ、YARNに提出されます。
- YARNは、この提出されたジョブに対して、ジョブを実行する必要なリソース（メモリ、CPUなど）を割り当てます。YARNはまず MrAppMaster（MapReduce Application Master）という実体を起動して、ジョブの周期を管理し、流れの実行順序を制定します。後のMapTask、ReduceTaskにジョブ処理がMrAppMasterの制御で仕上げます。

![image-20241223075836684](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20241223075836684.png)

**Map階段**

- Map階段に実行するロジックは書き直されたmap()メソッドが担当します。
- 入力の<key/value>値はドキュメントの行数keyと当の行の文字です。出力の<key/value>値が一つ単語textとその単語の計数1です。入力<key/value>値が新しい<key/value>値に転換になります。

![image-20241014162847885](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20241014162847885.png)

- mapの処理が完了した後、mapの各結果は`context.write`を通じてデータが収集されます。

**Collect階段**

- mapの処理が完了した後、mapの各結果は`context.write()`を通してデータが収集されます。それらのデータをキャッシュに一時的に格納します。
- でもその前に分類処理があります。データのkey値をハッシュ化（Hash）し、Reduceタスクの数で割って余りを取り、同じ余りによってデータを各タスクに割り当てます。
- そのために一つのMapTaskジョブに同じkeyを持つ<key/value>値が纏められます。後のデータ併合ReduceTaskがやすくなれます。その同時に全体の処理負荷を均等化することもできます。
- その一時的なキャッシュが環形キャッシュと呼ばれます。本質は配列で、key/valueペアとそのペアに対応する索引がそれぞれ格納されています。図中の2つの半円形矢印記号みたいものです。

![image-20250113204739841](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20250113204739841.png)

- 環形キャッシュは、デフォルト100Mサイズの制御があります。データを書き込む時に、一定の占有率（80%）に達したらこの部分がロックされて書き込みません。新しいプロセスがデータをキャッシュから取り出して別の所に転送します。その同時に書き込むプロセスが続きます。ただ、配列の末尾から逆方向に残りの20Mキャッシュに書き込みます。一旦80%閾値に達したら上記の過程を繰り返して先頭から書き込みます。
- 環形キャッシュが上述流れを抽象化にする表示であり、新しいデータの追加と古いデータの削除が同時に行われるため効率的になれます。

**Spill階段**

- Spill実は上記の環形キャッシュから書き出す流れです。環形キャッシュの80%閾値までになって80Mデータをロックして単独のプロセスを起動します。データを取り出して一時ファイルに書き込みます。

> spillが漢字で表せば「溢出」にでき、或いは「溢写」に書くと理解しやすい。

![image-20241225075625672](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20241225075625672.png)

- 臨時のファイルに書き込む前に、幾つかデータ処理があります。先ず、上図のパーティション1とパーティション2がkeyによって分けて出します。一つのパーティション（partition）が一種単語keyを代表します。例えば、パーティション1が今回Spill溢出の<a,1>を格納し、パーティション2が<b,1>を格納します。
- 次、さっき分けられたパーティションがkey/value値の索引によって並べ替えをします。
- 並べ替えられたデータを一時ファイルに書き込み、次のMerge階段に入ります。
- もしあるパーティションに対して出力結果が非常に大きい場合、ディスク上には複数の一時ファイルが存在することになります。

**Merge階段**

- そして、今回MapTaskの全てのデータ処理が終了した後に、ディスク上の一時ファイルを併合（merge）して一つにまとめます。
- 最終的には一つのファイルのみがディスクに書き込まれ、このファイルには各ReduceTaskに対応するデータのオフセット（offset）を記録するための索引も提供されます。

![image-20241225075713510](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20241225075713510.png)

　ここまでMapTaskの流れが終わりました。

### 7.2　MapTask並列処理

　MapTaskの並列処理とは、同時に実行されるMapTaskのタスク数を指し、Mapフェーズの並列処理能力を測る指標となります。この並列度を決定する要素は、read段階で、FileInputFormat`クラスの`getSplits()`メソッドが呼び出され、データブロックを論理的に分割して生成された切片（スプリット）数です。デフォルト場合で、切片のサイズとデータブロックのサイズが一致しています。このため、以下等式が成り立ちます：

- データブロックの数 ＝ 切片の数 ＝ MapTaskの数

　ここで例を挙げて切片を説明しましょう。

![image-20250216204508212](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20250216204508212.png)

　図1には、切片の大小とブロックが同じ、ブロックの全ては何も変わらなくて一つのタスクに入れて処理をします。図2は、100Mを単位として128Mデータが100Mと28Mに分けられました。残りの28Mデータが100Mに揃えておくため次のブロックに移動することが必要です。その過程には、最終タスク数が変わってないけど、データの移動があると余分な資源を消費するになります。

　その例を挙げて話したいのは切片大小を減ってタスク数が増えても実際に効率的になりません。ただ、ブロックと切片大小を一緒に縮めてタスク数は増えすぎを防ぐ上に適切に並列度を高めるとは、効率的にすることができます。

```
# hdfs-default.xml設定
dfs.blocksize=128MB

# mapred-default.xml配置ファイル
mapreduce.input.fileinputformat.split.maxsize=256MB
mapreduce.input.fileinputformat.split.minsize=64MB
```

　次、ソースコードを見て切片大小の判定ルールを紹介する。

　ブロックサイズが直接配置ファイルから得ってデフォルト値が128M、setBlockSize()みたいメソッドがないので、コードレベルに一時的に変更することができない。

![image-20250309112553210](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20250309112553210.png)

　切片サイズには、ブロック、切片の最大値と切片の最小値を比べて一つを選んで決める。computeSplitSizeメソッドから見えてブロックサイズをmaxSizeとminSize範囲を過ぎるにしては意味なく、決してその範囲にいる。その仕組みは切片サイズ範囲を制御して、多すぎまたは少なすぎタスクを生まれるせいで効率が下がることを防ぎ、ただその場合はデータ移動が生じてある。つまり、範囲内にの切片がブロック自身であり、範囲にブロックサイズを変えてタスク数が増えるとデータの移動もない。

![image-20250309112059668](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20250309112059668.png)

![image-20250309120204543](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20250309120204543.png)

　極端な状況に大規模データ、あるいは大量の小さなファイルに会うと、多すぎタスク数が生じるになる。block、minSize、maxSize三つを一緒に変えて、タスク数を減える同時にデータを移動することも起こってない。

![image-20250320161536287](D:\OneDrive\picture\Typora\BigData\Hadoop\image-20250320161536287.png)

　赤枠にソースコード表れるのは`bytesRemaining` が `splitSize` (切片サイズ) の 1.1 倍より大きい場合にのみ、一つの切片として分割される。ならば129Mのファイルも一つのブロックに入れる。

